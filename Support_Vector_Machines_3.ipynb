{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9B_EK4Me9ix"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd  # For data handling\n",
        "import numpy as np  # For numerical calculations\n",
        "import matplotlib.pyplot as plt  # For visualization\n",
        "from sklearn.svm import SVR  # For Support Vector Regression\n",
        "from sklearn.model_selection import train_test_split  # For splitting data\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error  # For evaluation metrics\n",
        "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
        "\n",
        "# Load the dataset\n",
        "# For this example, we'll assume the dataset is stored in a CSV file.\n",
        "# Replace the link with your actual dataset path\n",
        "# df = pd.read_csv(\"house_prices.csv\")  # Load dataset\n",
        "# Example dataset for illustration (assuming the dataset has 'features' and 'target')\n",
        "# df = pd.read_csv('path_to_your_dataset.csv')\n",
        "\n",
        "# 1. Define features and target variable\n",
        "# Assume columns: 'location', 'square_footage', 'bedrooms', 'price'\n",
        "# X contains the independent features, and y contains the dependent target (price)\n",
        "# X = df[['location', 'square_footage', 'bedrooms']]  # Features\n",
        "# y = df['price']  # Target variable\n",
        "\n",
        "# For the sake of the demonstration, we'll use a mock dataset\n",
        "X = np.random.rand(100, 3)  # Mock feature matrix (100 samples, 3 features)\n",
        "y = 50000 + 10000 * X[:, 0] + 5000 * X[:, 1] + 2000 * X[:, 2]  # Target variable (price)\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Preprocess the data: Scale features (important for SVM models)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data\n",
        "X_test_scaled = scaler.transform(X_test)  # Transform test data (to avoid data leakage)\n",
        "\n",
        "# 4. Build an SVM regression model\n",
        "# Using a linear kernel for simplicity, but can be adjusted for other kernels (e.g., RBF, polynomial)\n",
        "svm_regressor = SVR(kernel='linear')  # Use 'linear' kernel for simplicity\n",
        "svm_regressor.fit(X_train_scaled, y_train)  # Train the model\n",
        "\n",
        "# 5. Predictions using the trained model\n",
        "y_pred = svm_regressor.predict(X_test_scaled)  # Predict on test set\n",
        "\n",
        "# 6. Evaluate model performance using multiple regression metrics\n",
        "\n",
        "# **Q1: Best regression metric for predicting house price**\n",
        "# The best regression metric for house price prediction would typically be **Mean Squared Error (MSE)** or **R-squared (R²)**.\n",
        "# Let's calculate both and evaluate:\n",
        "\n",
        "# MSE: Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "\n",
        "# R²: Coefficient of determination (R-squared)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared (R²): {r2}\")\n",
        "\n",
        "# **Q2: Decision between MSE and R-squared for predicting actual price**\n",
        "# If the goal is to predict the actual price as accurately as possible, **MSE** would typically be a better choice as it penalizes large errors more than R².\n",
        "# MSE will give us an understanding of how far the predicted values are from the true values.\n",
        "\n",
        "# **Q3: Scenario with significant outliers**\n",
        "# If there are significant outliers, **Mean Absolute Error (MAE)** might be a better choice than MSE as it is less sensitive to large deviations caused by outliers.\n",
        "# Let's calculate MAE:\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "\n",
        "# **Q4: Choosing between MSE and RMSE when values are close**\n",
        "# RMSE (Root Mean Squared Error) and MSE are often very similar in value when they are close.\n",
        "# RMSE gives the error in the same units as the target variable, while MSE gives squared units.\n",
        "# If the values of MSE and RMSE are close, we may prefer **RMSE** as it is in the same units as the target (house price).\n",
        "\n",
        "# Calculate RMSE:\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "\n",
        "# **Q5: Evaluate the model with different kernels (Linear, Polynomial, RBF)**\n",
        "# To measure how well the model explains the variance in the target variable, we would primarily use **R-squared (R²)**.\n",
        "# The R² value explains the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
        "\n",
        "# Experimenting with other kernels (e.g., Polynomial, RBF) and evaluating with R²\n",
        "svm_poly = SVR(kernel='poly', degree=3)\n",
        "svm_poly.fit(X_train_scaled, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test_scaled)\n",
        "r2_poly = r2_score(y_test, y_pred_poly)\n",
        "print(f\"R-squared (R²) with Polynomial Kernel: {r2_poly}\")\n",
        "\n",
        "svm_rbf = SVR(kernel='rbf')\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "r2_rbf = r2_score(y_test, y_pred_rbf)\n",
        "print(f\"R-squared (R²) with RBF Kernel: {r2_rbf}\")\n",
        "\n",
        "# Conclusion:\n",
        "# - For predicting house prices, MSE is often the most informative metric, though R² is also widely used for understanding model performance.\n",
        "# - For datasets with outliers, MAE is often preferred.\n",
        "# - RMSE should be used when comparing performance metrics in the same units as the target.\n",
        "# - R-squared is the best metric to evaluate the proportion of explained variance, especially for models with different kernels.\n"
      ]
    }
  ]
}